
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import threading
import time
import gc
import os

# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–å†…å­˜åˆ†é…
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

class QwenChatBot:
    def __init__(self):
        self.model_name = "Qwen/Qwen3-0.6B"
        self.tokenizer = None
        self.model = None
        self.conversation_count = 0
        self.max_history_length = 10  # é™åˆ¶å†å²å¯¹è¯é•¿åº¦
        self.load_model()
    
    def clear_gpu_memory(self):
        """å¼ºåˆ¶æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
    
    def get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            free = total - reserved
            return {
                "allocated": allocated,
                "reserved": reserved, 
                "total": total,
                "free": free,
                "usage_percent": (reserved / total) * 100
            }
        return None
    
    def check_memory_pressure(self):
        """æ£€æŸ¥å†…å­˜å‹åŠ›ï¼Œå¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜åˆ™æ¸…ç†"""
        memory_info = self.get_gpu_memory_info()
        if memory_info and memory_info["usage_percent"] > 85:
            print(f"âš ï¸  High memory usage detected: {memory_info['usage_percent']:.1f}%")
            self.clear_gpu_memory()
            return True
        return False
    
    def load_model(self):
        """Load model and tokenizer with optimized settings"""
        print("Loading Qwen3-0.6B model with memory optimization...")
        
        # æ¸…ç†ç°æœ‰å†…å­˜
        self.clear_gpu_memory()
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦ä»¥èŠ‚çœå†…å­˜
            device_map="auto",
            low_cpu_mem_usage=True,     # å‡å°‘CPUå†…å­˜ä½¿ç”¨
            use_cache=True,             # å¯ç”¨KVç¼“å­˜ä½†æˆ‘ä»¬ä¼šä¸»åŠ¨ç®¡ç†
        )
        
        print("Model loaded successfully!")
        
        # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
        memory_info = self.get_gpu_memory_info()
        if memory_info:
            print(f"ğŸ“Š Initial GPU memory: {memory_info['allocated']:.2f}GB allocated, {memory_info['free']:.2f}GB free")
    
    def trim_history(self, history):
        """ä¿®å‰ªå†å²å¯¹è¯ä»¥èŠ‚çœå†…å­˜"""
        if len(history) > self.max_history_length:
            # ä¿ç•™æœ€è¿‘çš„å¯¹è¯
            trimmed = history[-self.max_history_length:]
            print(f"ğŸ”„ Trimmed conversation history from {len(history)} to {len(trimmed)} entries")
            return trimmed
        return history
    
    def generate_response(self, message, history, max_tokens=256, temperature=0.7, top_p=0.8):
        """Generate response with aggressive memory management"""
        try:
            # å¢åŠ å¯¹è¯è®¡æ•°å™¨
            self.conversation_count += 1
            
            # æ¯5æ¬¡å¯¹è¯å¼ºåˆ¶æ¸…ç†ä¸€æ¬¡å†…å­˜
            if self.conversation_count % 5 == 0:
                print("ğŸ§¹ Performing periodic memory cleanup...")
                self.clear_gpu_memory()
            
            # æ£€æŸ¥å†…å­˜å‹åŠ›
            self.check_memory_pressure()
            
            # ä¿®å‰ªå†å²å¯¹è¯
            history = self.trim_history(history)
            
            # Build message history
            messages = []
            
            # Add chat history (é™åˆ¶æ•°é‡)
            for user_msg, assistant_msg in history[-5:]:  # åªä¿ç•™æœ€è¿‘5è½®å¯¹è¯
                messages.append({"role": "user", "content": user_msg})
                messages.append({"role": "assistant", "content": assistant_msg})
            
            # Add current user message
            messages.append({"role": "user", "content": message})
            
            # Use official chat template, disable thinking mode for direct response
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False  # Disable thinking mode for direct response
            )
            
            print(f"ğŸ” Input text length: {len(text)} chars")
            
            # å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜
            self.clear_gpu_memory()
            
            # Encode input with memory optimization
            model_inputs = self.tokenizer(
                [text], 
                return_tensors="pt", 
                truncation=True,      # æˆªæ–­è¿‡é•¿è¾“å…¥
                max_length=1024,      # é™åˆ¶è¾“å…¥é•¿åº¦
            ).to(self.model.device)
            
            # é™ä½ç”Ÿæˆå‚æ•°ä»¥èŠ‚çœå†…å­˜
            generation_kwargs = {
                "max_new_tokens": min(max_tokens, 128),  # è¿›ä¸€æ­¥é™ä½æœ€å¤§tokenæ•°
                "temperature": temperature,
                "top_p": top_p,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": False,   # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
                "attention_mask": model_inputs.attention_mask,
            }
            
            print(f"ğŸ”§ Generation parameters: max_tokens={generation_kwargs['max_new_tokens']}")
            
            # æ˜¾ç¤ºç”Ÿæˆå‰å†…å­˜çŠ¶æ€
            memory_info = self.get_gpu_memory_info()
            if memory_info:
                print(f"ğŸ“Š Pre-generation GPU memory: {memory_info['allocated']:.2f}GB used, {memory_info['free']:.2f}GB free")
            
            # Generate response with proper memory management
            with torch.no_grad():
                generated_ids = self.model.generate(
                    input_ids=model_inputs.input_ids,
                    **generation_kwargs
                )
            
            # Extract generated tokens
            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
            
            # Decode response
            response = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
            
            print(f"âœ… Generated response length: {len(response)} chars")
            
            # ç«‹å³æ¸…ç†æ‰€æœ‰ä¸´æ—¶å˜é‡
            del model_inputs, generated_ids, output_ids
            self.clear_gpu_memory()
            
            # Simple cleanup
            response = response.strip()
            
            # Provide default response if empty
            if not response:
                if "hello" in message.lower() or "hi" in message.lower():
                    response = "Hello! I'm an AI assistant, nice to meet you. How can I help you today?"
                elif any(x in message.lower() for x in ["calculate", "math", "=", "+"]):
                    response = "I can help you with calculations. Please tell me the specific equation."
                else:
                    response = "I understand your question. Is there anything else I can help you with?"
                
            return response
            
        except torch.cuda.OutOfMemoryError as e:
            # å¤„ç†OOMé”™è¯¯
            print(f"ğŸ’¥ GPU OOM Error: {str(e)}")
            
            # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                torch.cuda.synchronize()
            gc.collect()
            
            # æ˜¾ç¤ºå†…å­˜çŠ¶æ€
            memory_info = self.get_gpu_memory_info()
            if memory_info:
                print(f"ğŸ“Š Post-cleanup GPU memory: {memory_info['allocated']:.2f}GB used, {memory_info['free']:.2f}GB free")
            
            error_msg = f"âŒ GPUå†…å­˜ä¸è¶³ã€‚è¯·å°è¯•ï¼š\n1. é™ä½æœ€å¤§ä»¤ç‰Œæ•°åˆ°64-128\n2. ç‚¹å‡»æ¸…ç©ºå¯¹è¯\n3. é‡å¯åº”ç”¨\n\nå½“å‰GPUä½¿ç”¨ç‡ï¼š{memory_info['usage_percent']:.1f}%" if memory_info else "âŒ GPUå†…å­˜ä¸è¶³ï¼Œè¯·é‡å¯åº”ç”¨ã€‚"
            return error_msg
        except Exception as e:
            error_msg = f"âŒ Error generating response: {str(e)}"
            print(error_msg)
            return error_msg
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯çŠ¶æ€å¹¶æ¸…ç†å†…å­˜"""
        self.conversation_count = 0
        self.clear_gpu_memory()
        print("ğŸ”„ Conversation reset and memory cleared")

# Create chatbot instance
chatbot = QwenChatBot()

def chat_fn(message, history, max_tokens, temperature, top_p):
    """Chat function with memory management"""
    if not message.strip():
        return history, ""
    
    # æ£€æŸ¥å†å²é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™å»ºè®®æ¸…ç©º
    if len(history) > 15:
        warning_msg = "âš ï¸ å¯¹è¯å†å²è¾ƒé•¿ï¼Œå»ºè®®ç‚¹å‡»'æ¸…ç©º'æŒ‰é’®é‡Šæ”¾å†…å­˜ã€‚"
        history.append(("ç³»ç»Ÿæç¤º", warning_msg))
        return history, ""
    
    # Generate response
    response = chatbot.generate_response(message, history, max_tokens, temperature, top_p)
    
    # Update history
    history.append((message, response))
    
    return history, ""

def clear_chat():
    """Clear chat history and reset conversation"""
    chatbot.reset_conversation()
    return [], ""

def show_memory_status():
    """æ˜¾ç¤ºå½“å‰å†…å­˜çŠ¶æ€"""
    memory_info = chatbot.get_gpu_memory_info()
    if memory_info:
        status = f"""
        ğŸ“Š **GPUå†…å­˜çŠ¶æ€**
        - å·²ä½¿ç”¨: {memory_info['allocated']:.2f}GB
        - å·²ä¿ç•™: {memory_info['reserved']:.2f}GB  
        - æ€»è®¡: {memory_info['total']:.2f}GB
        - ç©ºé—²: {memory_info['free']:.2f}GB
        - ä½¿ç”¨ç‡: {memory_info['usage_percent']:.1f}%
        """
        return status
    return "GPUå†…å­˜ä¿¡æ¯ä¸å¯ç”¨"

# Create Gradio Interface
with gr.Blocks(
    title="Qwen3-0.6B Chatbot (Memory Optimized)",
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        max-width: 900px !important;
        margin: auto !important;
    }
    """
) as demo:
    
    gr.Markdown(
        """
        # Qwen3-0.6B Chatbot (å†…å­˜ä¼˜åŒ–ç‰ˆ)
        
        Welcome to the intelligent chat assistant powered by Qwen3-0.6B! å·²ä¼˜åŒ–GPUå†…å­˜ç®¡ç†ã€‚
        
        > **å†…å­˜ä¼˜åŒ–ç‰¹æ€§**: 
        > - è‡ªåŠ¨å†…å­˜æ¸…ç†å’Œç›‘æ§
        > - æ™ºèƒ½å†å²å¯¹è¯ä¿®å‰ª
        > - é™ä½é»˜è®¤å‚æ•°å‡å°‘å†…å­˜ä½¿ç”¨
        > - OOMé”™è¯¯æ¢å¤æœºåˆ¶
        """
    )
    
    with gr.Row():
        with gr.Column(scale=3):
            # èŠå¤©ç•Œé¢
            chatbot_ui = gr.Chatbot(
                height=500,
                show_label=False,
                container=False,
                show_copy_button=True
            )
            
            with gr.Row():
                msg_input = gr.Textbox(
                    placeholder="Type your message here...",
                    show_label=False,
                    scale=4,
                    container=False
                )
                send_btn = gr.Button("Send ğŸ“¤", scale=1, variant="primary")
                clear_btn = gr.Button("Clear ğŸ—‘ï¸", scale=1, variant="secondary")
        
        with gr.Column(scale=1):
            gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•° (å†…å­˜ä¼˜åŒ–)")
            
            max_tokens = gr.Slider(
                minimum=32,
                maximum=256,      # é™ä½æœ€å¤§å€¼
                value=128,        # é™ä½é»˜è®¤å€¼
                step=16,
                label="Max Tokens",
                info="å“åº”æœ€å¤§é•¿åº¦ (å·²ä¼˜åŒ–)"
            )
            
            temperature = gr.Slider(
                minimum=0.1,
                maximum=1.5,      # é™ä½æœ€å¤§å€¼
                value=0.7,
                step=0.1,
                label="Temperature",
                info="åˆ›é€ æ€§æ§åˆ¶"
            )
            
            top_p = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.8,
                step=0.05,
                label="Top-p",
                info="è¯æ±‡å¤šæ ·æ€§æ§åˆ¶"
            )
            
            # å†…å­˜çŠ¶æ€æ˜¾ç¤º
            memory_status = gr.Markdown(show_memory_status())
            refresh_memory_btn = gr.Button("åˆ·æ–°å†…å­˜çŠ¶æ€ ğŸ”„", size="sm")
            
            gr.Markdown(
                """
                ### ğŸ’¡ å†…å­˜ä¼˜åŒ–æç¤º
                - **Max Tokens**: å»ºè®®ä½¿ç”¨64-128ä»¥èŠ‚çœå†…å­˜
                - **é•¿å¯¹è¯**: è¶…è¿‡15è½®åå»ºè®®æ¸…ç©ºå†å²
                - **OOMé”™è¯¯**: é™ä½å‚æ•°æˆ–é‡å¯åº”ç”¨
                
                ### ğŸ¯ æ¨èè®¾ç½®
                - æ—¥å¸¸å¯¹è¯: Tokens=64, Temp=0.7
                - åˆ›æ„å†™ä½œ: Tokens=128, Temp=1.0  
                - é—®ç­”å¯¹è¯: Tokens=96, Temp=0.5
                """
            )
    
    # äº‹ä»¶ç»‘å®š
    send_btn.click(
        chat_fn,
        inputs=[msg_input, chatbot_ui, max_tokens, temperature, top_p],
        outputs=[chatbot_ui, msg_input]
    )
    
    msg_input.submit(
        chat_fn,
        inputs=[msg_input, chatbot_ui, max_tokens, temperature, top_p],
        outputs=[chatbot_ui, msg_input]
    )
    
    clear_btn.click(clear_chat, outputs=[chatbot_ui, msg_input])
    
    refresh_memory_btn.click(
        lambda: show_memory_status(),
        outputs=[memory_status]
    )

if __name__ == "__main__":
    print("ğŸŒŸ Starting Qwen3-0.6B Chat Application (Memory Optimized)...")
    print("ğŸ”§ Memory optimization features enabled:")
    print("   - Expandable segments for reduced fragmentation")
    print("   - Aggressive memory cleanup")
    print("   - Lower default parameters")
    print("   - History length limiting")
    
    demo.launch(
        server_name="0.0.0.0",  # Allow external access
        server_port=7860,
        share=False,  # Set to True to get public link
        show_error=True
    ) 